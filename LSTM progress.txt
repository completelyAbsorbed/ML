# LSTM guide
# https://machinelearningmastery.com/long-short-term-memory-recurrent-neural-networks-mini-course/


Lesson 1 : What are LSTMs?

What is sequence prediction and what are some general examples?
	- Sequence Prediction is a problem that involves using historical sequence information to predict the next value or values in the sequence.
	- image captioning, sequence classification, sequence generation, sequence to sequence prediction
What are the limitations of traditional neural networks for sequence prediction?
	- the mapping function is static
		- number of inputs and outputs are fixed
		- have to specify the temporal dependence upfront in the design of the model
What is the promise of RNNs for sequence prediction?
	- the promise of RNNs for sequence prediction is that the temporal dependence in the input data can be learned,
		that a fixed set of lagged observations does not need to be specifed
What is the LSTM and what are its constituent parts?
	- an LSTM is a RNN that uses constant error carrousels (CEC) to escape exploding/vanishing gradients
	- LSTM layers consist of reccurently connected blocks, known as memory blocks. 
		- each memory block contains one or more reccurently connected memory cells and three multiplicative units:
		- input, output, forget gates
		- the net can only interact with the cells via the gates
What are some prominent applications of LSTMs?
	- language modeling, speech translation, machine translation, 
		problems in which we suspect that a hierarchical decomposition may exist, but do not know in advance what this decomposition is
	
Lesson 2: How LSTMs are trained

Goal

The goal of this lesson is to understand how LSTM models are trained on example sequences.

Questions

What common problems afflict the training of traditional RNNs?
	- exploding/vanishing gradients
How does the LSTM overcome these problems?
	- constant error carrousels (CEC) and BPTT : Backpropagation Through Time
What algorithm is used to train LSTMs?
	- BPTT
How does Backpropagation Through Time work?
	- unrolls the network, calculates errors across each timestep, update weights
What is truncated BPTT and what benefit does it offer?
	- is like BPTT but changing chunks used for training, speeds up training time. may miss longer-term 
How is BPTT implemented and configured in Keras?	
	- k1=k2=h, h<n, 10s to 100s of timesteps
	
	
Lesson 3: How to prepare data for LSTMs

Goal

The goal of this lesson is to understand how to prepare sequence prediction data for use with LSTM models.

Questions

How do you prepare numeric data for use with LSTMs?
	- scaling or standardization using MinMaxScaler or StandardScaler, depending on underlying distribution assumptions
How do you prepare categorical data for use with LSTMs?
	- integer encoded, then "one hot encoded"
	so, make a unique integer for each category for that variable, then make that variable column into
		a number of binary columns, having encoded those integers to 1s and 0s to fit in those places
How do you handle missing values in sequences when using LSTMs?
	- remove their row
	- or, fill them in with another value, such as one not naturally appearing in the series(perhaps -1? encouraging model to learn 'missing' flag)
		- or, learn missing values!
		- we can also use a Masking layer at the front of the LSTM to mask (-1 or whichever value) to automatically avoid marked missing rows
How do you frame a sequence as a supervised learning problem?
	- Convert from sequence of numbers ordered by a time sequence (ie a Time Series) to a pair of input patterns(X) and output patterns(y) (ie Supervised Learning)
	- use pandas shift
How do you handle long sequences when working with LSTMs?
	- in practice ML engineers use a reasonable limit of 250-500 timesteps
	- truncating sequences : cutting from beginning or end of input sequences
	- summarizing : different ways to do this ex. removing most frequent words like 'the', or open/high/low/close for ticker data
	- random sampling
	- truncated backpropagation through time (TBPTT) : included some places, eg. "truncate_gradient" argument in Theano
	- use an Encoder-Decoder architecture : 
How do you handle input sequences with different lengths?
	- padding using pad_sequences() in Keras. 
	- truncation using pad_sequences(sequences, maxlen=n)
How do you reshape input data for LSTMs in Keras?
	- use NumPy function data.reshape() to make the data 3D
		- Samples
		- Time Steps
		- Features
	- can add input_shape(a,b) parameter to model.add(LSTM(x, input_shape(a,b)) 
		- a specifies number of time steps
		- b specifies number of features
		- a and b are not the actual parameter names for input_shape

Experiment

Demonstrate how to transform a numerical input sequence into a form suitable for training an LSTM.


Lesson 4: How to develop LSTMs in Keras

Goal

The goal of this lesson is to understand how to define, fit, and evaluate LSTM models using the Keras deep learning library in Python.

Questions

How do you define an LSTM Model?
	- 
How do you compile an LSTM Model?
	- 
How do you fit an LSTM Model?
	- 
How do you evaluate an LSTM Model?
	- 
How do you make predictions with an LSTM Model?
	- 
How can LSTMs be applied to different types of sequence prediction problems?
	- 
Experiment

Prepare an example that demonstrates the life-cycle of an LSTM model on a sequence prediction problem.






















