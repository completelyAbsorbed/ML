# copy/pasted (then formatted) from https://people.duke.edu/~rnau/arimrule.htm
Identifying the order of differencing and the constant:

Rule 1: If the series has positive autocorrelations out to a high number of lags (say, 10 or more), 
	then it probably needs a higher order of differencing.
Rule 2: If the lag-1 autocorrelation is zero or negative, or the autocorrelations are all small and patternless, 
	then the series does not need a higher order of differencing. 
	If the lag-1 autocorrelation is -0.5 or more negative, the series may be overdifferenced.  BEWARE OF OVERDIFFERENCING.
Rule 3: The optimal order of differencing is often the order of differencing at which the standard deviation is lowest. 
	(Not always, though. Slightly too much or slightly too little differencing can also be corrected with AR or MA terms. 
	See rules 6 and 7.)
Rule 4: A model with no orders of differencing assumes that the original series is stationary 
	(among other things, mean-reverting). 
	A model with one order of differencing assumes that the original series has a constant average trend 
	(e.g. a random walk or SES-type model, with or without growth). 
	A model with two orders of total differencing assumes that the original series has a time-varying trend 
	(e.g. a random trend or LES-type model).
Rule 5: A model with no orders of differencing normally includes a constant term (which allows for a non-zero mean value). 
	A model with two orders of total differencing normally does not include a constant term. 
	In a model with one order of total differencing, a constant term should be included if the series has a non-zero average 
	trend.

Identifying the numbers of AR and MA terms:

Rule 6: If the partial autocorrelation function (PACF) of the differenced series displays a sharp cutoff and/or 
	the lag-1 autocorrelation is positive--i.e., if the series appears slightly "underdifferenced"--then consider 
	adding one or more AR terms to the model. The lag beyond which the PACF cuts off is the indicated number of AR terms.
Rule 7: If the autocorrelation function (ACF) of the differenced series displays 
	a sharp cutoff and/or the lag-1 autocorrelation is negative--i.e., if the series appears slightly "overdifferenced"--then
	consider adding an MA term to the model. The lag beyond which the ACF cuts off is the indicated number of MA terms.
Rule 8: It is possible for an AR term and an MA term to cancel each other's effects, 
	so if a mixed AR-MA model seems to fit the data, also try a model with one fewer AR term and one fewer MA term
	--particularly if the parameter estimates in the original model require more than 10 iterations to converge. 
	BEWARE OF USING MULTIPLE AR TERMS AND MULTIPLE MA TERMS IN THE SAME MODEL.
Rule 9: If there is a unit root in the AR part of the model--i.e., if the sum of the AR coefficients is almost exactly 1
	--you should reduce the number of AR terms by one and increase the order of differencing by one.
Rule 10: If there is a unit root in the MA part of the model--i.e., if the sum of the MA coefficients is almost exactly 1
	--you should reduce the number of MA terms by one and reduce the order of differencing by one.
Rule 11: If the long-term forecasts* appear erratic or unstable, there may be a unit root in the AR or MA coefficients.

Identifying the seasonal part of the model:

Rule 12: If the series has a strong and consistent seasonal pattern, then you must 
	use an order of seasonal differencing (otherwise the model assumes that the seasonal pattern will fade away over time). 
	However, never use more than one order of seasonal differencing or more than 2 orders of total differencing 
	(seasonal+nonseasonal).
Rule 13: If the autocorrelation of the appropriately differenced series is positive at lag s, 
	where s is the number of periods in a season, then consider adding an SAR term to the model. 
	If the autocorrelation of the differenced series is negative at lag s, consider adding an SMA term to the model. 
	The latter situation is likely to occur if a seasonal difference has been used, which should be done if the data
	has a stable and logical seasonal pattern. The former is likely to occur if a seasonal difference has not been used, 
	which would only be appropriate if the seasonal pattern is not stable over time. 
	You should try to avoid using more than one or two seasonal parameters (SAR+SMA) in the same model, 
	as this is likely to lead to overfitting of the data and/or problems in estimation.

*A caveat about long-term forecasting in general: linear time series models such as ARIMA and exponential smoothing models
	predict the more distant future by making a series of one-period-ahead forecasts and plugging them in for
	unknown future values as they look farther ahead. For example, a 2-period-ahead forecast is computed by treating the 
	1-period-ahead forecast as if it were data and then applying the same forecasting equation. 
	This step can be repeated any number of times in order to forecast as far into the future as you want, 
	and the method also yields formulas for computing theoretically-appropriate confidence intervals around the 
	longer-term forecasts. However, the models are identified and optimized based on their one-period-ahead forecasting 
	performance, and rigid extrapolation of them may not be the best way to forecast many periods ahead 
	(say, more than one year when working with monthly or quarterly business data), 
	particularly when the modeling assumptions are at best only approximately satisfied 
	(which is nearly always the case). If one of your objectives is to generate long-term forecasts, 
	it would be good to also draw on other sources of information during the model selection process and/or to optimize the
	parameter estimates for multi-period forecasting if your software allows it and/or use an auxiliary model 
	(possibly one that incorporates expert opinion) for long-term forecasting.
